{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "\n",
    "# Load the YAML configuration\n",
    "with open(\"configs/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Extract parameters\n",
    "frac = config['data_split']['frac']\n",
    "test_size = config['data_split']['test_size']\n",
    "\n",
    "\n",
    "y = train_labels.sample(frac=frac, random_state=1)\n",
    "x = train_features.loc[y.index].filepath.to_frame()\n",
    "\n",
    "# note that we are casting the species labels to an indicator/dummy matrix\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(\n",
    "    x, y, stratify=y, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    \"\"\"Reads in an image, transforms pixel values, and serves\n",
    "    a dictionary containing the image id, image tensors, and label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x_df, y_df=None):\n",
    "        self.data = x_df\n",
    "        self.label = y_df\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.data.iloc[index][\"filepath\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        image_id = self.data.index[index]\n",
    "        # if we don't have labels (e.g. for test set) just return the image and image id\n",
    "        if self.label is None:\n",
    "            sample = {\"image_id\": image_id, \"image\": image}\n",
    "        else:\n",
    "            label = torch.tensor(self.label.iloc[index].values, \n",
    "                                 dtype=torch.float)\n",
    "            sample = {\"image_id\": image_id, \"image\": image, \"label\": label}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the YAML configuration\n",
    "with open(\"configs/config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "    \n",
    "batch_size = config['data_preprocess']['batch_size']\n",
    "\n",
    "train_dataset = ImagesDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset\n",
    "    ,batch_size=batch_size # can be adjusted in yaml\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1280, 100),  # 1280-dim input corresponds to EfficientNet-B0\n",
    "    nn.ReLU(inplace=True),  # ReLU activation introduces non-linearity\n",
    "    nn.Dropout(0.1),  # Dropout to mitigate overfitting\n",
    "    nn.Linear(100, 8)  # Final dense layer outputs 8 classes\n",
    ")\n",
    "\n",
    "# Optionally freeze the backbone weights\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the backbone if needed, after training the head\n",
    "# Adjust the learning rate accordingly during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = config['optimizer_hyper_p']['lr']\n",
    "momentum = config['optimizer_hyper_p']['momentum']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters()\n",
    "    ,lr=lr\n",
    "    ,momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "tracking_loss = {}\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Starting epoch {epoch}\")\n",
    "\n",
    "    # iterate through the dataloader batches. tqdm keeps track of progress.\n",
    "    for batch_n, batch in tqdm(\n",
    "        enumerate(train_dataloader), total=len(train_dataloader)\n",
    "    ):\n",
    "\n",
    "        # 1) zero out the parameter gradients so that gradients from previous batches are not used in this step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2) run the foward step on this batch of images\n",
    "        outputs = model(batch[\"image\"])\n",
    "\n",
    "        # 3) compute the loss\n",
    "        loss = criterion(outputs, batch[\"label\"])\n",
    "        # let's keep track of the loss by epoch and batch\n",
    "        tracking_loss[(epoch, batch_n)] = float(loss)\n",
    "\n",
    "        # 4) compute our gradients\n",
    "        loss.backward()\n",
    "        # update our weights\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
